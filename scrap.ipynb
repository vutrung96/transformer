{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe1630c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-03 16:15:45.752229: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-03 16:15:45.752255: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-03 16:15:45.753219: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-03 16:15:46.303504: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "from typing import Any, Callable, Sequence\n",
    "from jax import random, numpy as jnp\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "import jax.numpy as jnp\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from clu import metrics\n",
    "from flax.training import train_state  # Useful dataclass to keep train state\n",
    "from flax import struct                # Flax dataclasses\n",
    "import optax                           # Common loss functions and optimizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9fa4b33-454e-4507-8839-0bd124a82769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a89d9071-f990-4396-bbf5-5ba8b83d7573",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tfds.load('wikipedia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80aff333-0f7f-4c35-9285-ab2736be3819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec={'text': TensorSpec(shape=(), dtype=tf.string, name=None), 'title': TensorSpec(shape=(), dtype=tf.string, name=None)}>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "402e3f4e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AutoTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Tokenize using Hugging Face tokenizer\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mencode(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai-gpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m BATCH_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[1;32m     11\u001b[0m VOCAB_SIZE \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mvocab_size\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AutoTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "@tf.py_function(Tout=tf.int32)\n",
    "def tokenize_py_func(text):\n",
    "    # Convert tensor to numpy array\n",
    "    text = text.numpy().decode()\n",
    "    # Tokenize using Hugging Face tokenizer\n",
    "    return tokenizer.encode(text, return_tensors=\"tf\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-gpt\")\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "\n",
    "d = tfds.load('wikipedia')['train']\n",
    "d = (d\n",
    "     .map(lambda x: x['text'], num_parallel_calls=tf.data.AUTOTUNE)\n",
    "     .map(tokenize_py_func, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "     .map(lambda x: {'x': x[0][:-1], 'y': tf.roll(x[0], -1, 0)[:-1]}, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "     .unbatch()\n",
    "     .shuffle(1024)\n",
    "     .batch(BATCH_SIZE)\n",
    "     .prefetch(1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "82961eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2376 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': <tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
      "array([ 2720, 14085,   481, 14085,  7649,   544,  8312,   244, 10945,\n",
      "        5722,   485,   488,  4640,   488,  4240,   729,   240,   872,\n",
      "         481,  7598,   488, 32965,   239,  2672, 20733,   260, 26836,\n",
      "       36792,  3690,  7649, 15959, 22772], dtype=int32)>, 'y': <tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
      "array([  260,  8312,  3779,   270, 13057,   604,   485,   240,   498,\n",
      "        1780, 35351,   246, 16667,  1964,   617,  2216,   488,   498,\n",
      "        2720, 19290,  7271,   275, 39475, 14319, 15776,  5699,     0,\n",
      "         500,   872, 24476,  7271,   831], dtype=int32)>}\n",
      "{'x': <tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
      "array([  725, 33410,  4353,   725, 35351,   556,  6350,   928,  1026,\n",
      "       15827,   500, 34575,   630,  2445, 33490,  1474,   562,   616,\n",
      "         568,   522,  1147,   246,   481,   481,   765, 19155,  3706,\n",
      "       15018, 35351,   500,   485, 15186], dtype=int32)>, 'y': <tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
      "array([  239, 14085,   481,  5112,  3682,   481,  1098,   781,  1389,\n",
      "         485,   905,  6283,   557,  1878,   500,     7,   246,  5883,\n",
      "         725,   246,  7598, 38961,  5870,  1621,   276,  3449,   239,\n",
      "         239,  3682,  8898,  1973,   498], dtype=int32)>}\n",
      "{'x': <tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
      "array([ 1276,   525,   866,   276,   987, 35351,   240,  7271,  6112,\n",
      "         999,   246,   240,  4240,  1182,  7912,  7271,  2314,   500,\n",
      "         485,   636,   485,  7453, 10139,   630, 18905,   522,  5646,\n",
      "         589, 10283, 40195,   239,   617], dtype=int32)>, 'y': <tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
      "array([  641, 35351,   640, 12568, 11764,   523,   905, 22772,  1208,\n",
      "         260, 17703,  1000,    16,  1210,   781, 22772,  8452,   481,\n",
      "        5701,   580,   729,   481,  2557,   246,   557, 35351, 16316,\n",
      "       18530,   488,   240,  4640, 14245], dtype=int32)>}\n",
      "{'x': <tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
      "array([26419, 29846, 11679, 37323,   240,   523, 35351,  1681,   481,\n",
      "         244,   488,   980,  7271,  4258,   246,   478,  1208,  1427,\n",
      "         244,   498,   522,  3682,  5870,   987,  2528,   240,  1944,\n",
      "         481, 22078,  7467, 28944, 10824], dtype=int32)>, 'y': <tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
      "array([ 3106,  7739,    36,  5515,  7271, 19501,  3682,   246, 32024,\n",
      "        1147, 13515,   694, 22772,  1109,   240, 19154,  7116,   504,\n",
      "         522,  6797,   754,  9990, 22799,  1529,   694,   481,   239,\n",
      "        7649, 37323,   256,  5031,   485], dtype=int32)>}\n",
      "{'x': <tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
      "array([35351,   488,  1805,  1252, 18530,  3682,   556,   276,  4719,\n",
      "       22772,   523,   694, 35351, 19290,  1386,  3706,   240, 16331,\n",
      "         646,  8923,   754,  2672,   488,   557,   240,   240,     0,\n",
      "         595,   548,   239,   617,   498], dtype=int32)>, 'y': <tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
      "array([ 3682,   666,   593,   485,   240,   239, 10139,   275,   525,\n",
      "        1386,   866, 12792,   523,  5722,   498, 33490,   488,  1964,\n",
      "        2908,    10, 18852, 12371,  7271,  4111, 22925,   522,  8083,\n",
      "       14125,  1707,   556,  1952, 35351], dtype=int32)>}\n"
     ]
    }
   ],
   "source": [
    "for e in d.take(5):\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa4be9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax.linen as nn\n",
    "import jax, jax.numpy as jnp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "d06192f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'params': {'embedding': (40478, 64)}}"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = nn.Embed(num_embeddings=VOCAB_SIZE, features=64)\n",
    "params = layer.init(jax.random.key(0), jnp.ones((1, 5), dtype=int))\n",
    "jax.tree_util.tree_map(lambda x: x.shape, params) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1982b760",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "37ed6f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnigramModel(nn.Module):\n",
    "    vocab_size: int\n",
    "    embedding_dim: int\n",
    "        \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Embed(num_embeddings=self.vocab_size, features=self.embedding_dim, name=\"unigram\")(x)\n",
    "        x = nn.Dense(features=self.vocab_size)(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4353133",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "7a8167c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UnigramModel(vocab_size=VOCAB_SIZE, embedding_dim=64)\n",
    "key1, key2 = random.split(random.key(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "23e3fd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@struct.dataclass\n",
    "class Metrics(metrics.Collection):\n",
    "    accuracy: metrics.Accuracy\n",
    "    loss: metrics.Average.from_output('loss')\n",
    "\n",
    "class TrainState(train_state.TrainState):\n",
    "    metrics: Metrics\n",
    "\n",
    "def create_train_state(module, rng, learning_rate, momentum):\n",
    "    \"\"\"Creates an initial `TrainState`.\"\"\"\n",
    "    dummy = random.randint(key1, minval=0, maxval=10, shape=(10,))\n",
    "    params = module.init(rng, dummy)['params'] # initialize parameters by passing a template image\n",
    "    tx = optax.sgd(learning_rate, momentum)\n",
    "    return TrainState.create(\n",
    "      apply_fn=module.apply, params=params, tx=tx,\n",
    "      metrics=Metrics.empty())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "71c653af",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(state, batch):\n",
    "    \"\"\"Train for a single step.\"\"\"\n",
    "    def loss_fn(params):\n",
    "        logits = state.apply_fn({'params': params}, batch['x'])\n",
    "        loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "            logits=logits, labels=batch['y']).mean()\n",
    "        return loss\n",
    "    grad_fn = jax.grad(loss_fn)\n",
    "    grads = grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state\n",
    "\n",
    "@jax.jit\n",
    "def compute_metrics(*, state, batch):\n",
    "    logits = state.apply_fn({'params': state.params}, batch['x'])\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "        logits=logits, labels=batch['y']).mean()\n",
    "    metric_updates = state.metrics.single_from_model_output(\n",
    "    logits=logits, labels=batch['y'], loss=loss)\n",
    "    metrics = state.metrics.merge(metric_updates)\n",
    "    state = state.replace(metrics=metrics)\n",
    "    return state\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def pred_step(state, x, key, T=10):\n",
    "    logits = state.apply_fn({'params': state.params}, x)\n",
    "    return jax.random.categorical(key, logits/T, axis=1)\n",
    "\n",
    "def generate(init_word, num_steps, key, T=10):\n",
    "    w = init_word\n",
    "    s = w\n",
    "    for i in range(num_steps):\n",
    "        key, _ = random.split(key)\n",
    "        w = tokenizer.decode(pred_step(state, np.array(tokenizer.encode(w)), key, T))\n",
    "        s += (' ' + w)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "44c011d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "f09487a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1437271066   43563307]\n"
     ]
    }
   ],
   "source": [
    "key = random.PRNGKey(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "id": "ee32e789",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[552], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m batch \u001b[38;5;241m=\u001b[39m d_iter\u001b[38;5;241m.\u001b[39mnext()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Run optimization steps over training batches and compute batch metrics\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# get updated train state (which contains the updated parameters)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m state \u001b[38;5;241m=\u001b[39m compute_metrics(state\u001b[38;5;241m=\u001b[39mstate, batch\u001b[38;5;241m=\u001b[39mbatch) \u001b[38;5;66;03m# aggregate batch metrics\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (step\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;66;03m# one training epoch has passed\u001b[39;00m\n",
      "File \u001b[0;32m<string>:1\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(_cls, trace)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "init_rng = jax.random.key(0)\n",
    "state = create_train_state(model, init_rng, learning_rate, momentum)\n",
    "metrics_history = {'train_loss': [],\n",
    "                   'train_accuracy': []}\n",
    "\n",
    "num_steps = 100000\n",
    "d_iter = d.as_numpy_iterator()\n",
    "for step in range(num_steps):\n",
    "    batch = d_iter.next()\n",
    "    # Run optimization steps over training batches and compute batch metrics\n",
    "    state = train_step(state, batch) # get updated train state (which contains the updated parameters)\n",
    "    state = compute_metrics(state=state, batch=batch) # aggregate batch metrics\n",
    "\n",
    "    if (step+1) % 1000 == 0: # one training epoch has passed\n",
    "        for metric,value in state.metrics.compute().items(): # compute metrics\n",
    "            metrics_history[f'train_{metric}'].append(value) # record metrics\n",
    "        state = state.replace(metrics=state.metrics.empty()) # reset train_metrics for next training epoch\n",
    "\n",
    "        print(f\"step: {(step+1)}, \"\n",
    "          f\"loss: {metrics_history['train_loss'][-1]}, \"\n",
    "          f\"accuracy: {metrics_history['train_accuracy'][-1] * 100}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "id": "f2715b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello dir passionately deduce stank venice ceremonial integrity advertise donkey whichever lifting avi goo exchanges negro bean jonas mutton leaner iette\n"
     ]
    }
   ],
   "source": [
    "key = random.PRNGKey(1)\n",
    "print(generate('hello', 20, key, T=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "e88c6a17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3570]"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "42cc7c33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[-0.,  0.,  0., ...,  0., -0., -0.],\n",
       "       [ 0.,  0.,  0., ...,  0., -0.,  0.],\n",
       "       [ 0., -0.,  0., ..., -0., -0.,  0.],\n",
       "       ...,\n",
       "       [-0., -0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0., -0.,  0., ...,  0.,  0., -0.],\n",
       "       [ 0., -0.,  0., ...,  0., -0., -0.]], dtype=float32)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.nnmodel.apply(params, x) * jax.nn.one_hot(y, VOCAB_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "fc530115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "87d0a2dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 40478)"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "3f856bbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([2, 1, 7, 7, 3, 5, 7, 8, 2, 2], dtype=int32)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
